{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "#### Optional: Run this cell to see available notebook commands (\"magics\").\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%help",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 0.38.1 \n",
					"output_type": "stream"
				},
				{
					"output_type": "display_data",
					"data": {
						"text/markdown": "\n# Available Magic Commands\n\n## Sessions Magic\n\n----\n    %help                             Return a list of descriptions and input types for all magic commands. \n    %profile            String        Specify a profile in your aws configuration to use as the credentials provider.\n    %region             String        Specify the AWS region in which to initialize a session. \n                                      Default from ~/.aws/config on Linux or macOS, \n                                      or C:\\Users\\ USERNAME \\.aws\\config\" on Windows.\n    %idle_timeout       Int           The number of minutes of inactivity after which a session will timeout. \n                                      Default: 2880 minutes (48 hours).\n    %session_id_prefix  String        Define a String that will precede all session IDs in the format \n                                      [session_id_prefix]-[session_id]. If a session ID is not provided,\n                                      a random UUID will be generated.\n    %status                           Returns the status of the current Glue session including its duration, \n                                      configuration and executing user / role.\n    %session_id                       Returns the session ID for the running session. \n    %list_sessions                    Lists all currently running sessions by ID.\n    %stop_session                     Stops the current session.\n    %glue_version       String        The version of Glue to be used by this session. \n                                      Currently, the only valid options are 2.0, 3.0 and 4.0. \n                                      Default: 2.0.\n----\n\n## Selecting Job Types\n\n----\n    %streaming          String        Sets the session type to Glue Streaming.\n    %etl                String        Sets the session type to Glue ETL.\n    %glue_ray           String        Sets the session type to Glue Ray.\n----\n\n## Glue Config Magic \n*(common across all job types)*\n\n----\n\n    %%configure         Dictionary    A json-formatted dictionary consisting of all configuration parameters for \n                                      a session. Each parameter can be specified here or through individual magics.\n    %iam_role           String        Specify an IAM role ARN to execute your session with.\n                                      Default from ~/.aws/config on Linux or macOS, \n                                      or C:\\Users\\%USERNAME%\\.aws\\config` on Windows.\n    %number_of_workers  int           The number of workers of a defined worker_type that are allocated \n                                      when a session runs.\n                                      Default: 5.\n    %additional_python_modules  List  Comma separated list of additional Python modules to include in your cluster \n                                      (can be from Pypi or S3).\n    %%tags        Dictionary          Specify a json-formatted dictionary consisting of tags to use in the session.\n----\n\n                                      \n## Magic for Spark Jobs (ETL & Streaming)\n\n----\n    %worker_type        String        Set the type of instances the session will use as workers. \n                                      ETL and Streaming support G.1X, G.2X, G.4X and G.8X. \n                                      Default: G.1X.\n    %connections        List          Specify a comma separated list of connections to use in the session.\n    %extra_py_files     List          Comma separated list of additional Python files From S3.\n    %extra_jars         List          Comma separated list of additional Jars to include in the cluster.\n    %spark_conf         String        Specify custom spark configurations for your session. \n                                      E.g. %spark_conf spark.serializer=org.apache.spark.serializer.KryoSerializer\n----\n                                      \n## Magic for Ray Job\n\n----\n    %min_workers        Int           The minimum number of workers that are allocated to a Ray job. \n                                      Default: 1.\n    %object_memory_head Int           The percentage of free memory on the instance head node after a warm start. \n                                      Minimum: 0. Maximum: 100.\n    %object_memory_worker Int         The percentage of free memory on the instance worker nodes after a warm start. \n                                      Minimum: 0. Maximum: 100.\n----\n\n## Action Magic\n\n----\n\n    %%sql               String        Run SQL code. All lines after the initial %%sql magic will be passed\n                                      as part of the SQL code.  \n----\n\n"
					},
					"metadata": {}
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "####  Run this cell to set up and start your interactive session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 2880\n%glue_version 3.0\n%worker_type G.1X\n%number_of_workers 5\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Current idle_timeout is 2800 minutes.\nidle_timeout has been set to 2880 minutes.\nSetting Glue version to: 3.0\nPrevious worker type: G.1X\nSetting new worker type to: G.1X\nPrevious number of workers: 5\nSetting new number of workers to: 5\nAuthenticating with environment variables and user-defined glue_role_arn: arn:aws:iam::755670014224:role/aws-glue-sessions-ii\nTrying to create a Glue session for the kernel.\nWorker Type: G.1X\nNumber of Workers: 5\nSession ID: 123f6aa8-8299-43e3-8173-c30477d891b0\nJob Type: glueetl\nApplying the following default arguments:\n--glue_kernel_version 0.38.1\n--enable-glue-datacatalog true\nWaiting for session 123f6aa8-8299-43e3-8173-c30477d891b0 to get into ready status...\nSession 123f6aa8-8299-43e3-8173-c30477d891b0 has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "import pyspark\nimport pandas as pd\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Define the context",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "sc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "raw",
			"source": "#### Example: Create a DynamicFrame from a table in the AWS Glue Data Catalog and display its schema\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "raw",
			"source": "dyf = glueContext.create_dynamic_frame.from_catalog(database='database_name', table_name='table_name')\ndyf.printSchema()",
			"metadata": {
				"editable": true
			}
		},
		{
			"cell_type": "raw",
			"source": "#### Example: Convert the DynamicFrame to a Spark DataFrame and display a sample of the data\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "raw",
			"source": "df = dyf.toDF()\ndf.show()",
			"metadata": {
				"editable": true
			}
		},
		{
			"cell_type": "raw",
			"source": "#### Example: Write the data in the DynamicFrame to a location in Amazon S3 and a table for it in the AWS Glue Data Catalog\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "raw",
			"source": "s3output = glueContext.getSink(\n  path=\"s3://bucket_name/folder_name\",\n  connection_type=\"s3\",\n  updateBehavior=\"UPDATE_IN_DATABASE\",\n  partitionKeys=[],\n  compression=\"snappy\",\n  enableUpdateCatalog=True,\n  transformation_ctx=\"s3output\",\n)\ns3output.setCatalogInfo(\n  catalogDatabase=\"demo\", catalogTableName=\"populations\"\n)\ns3output.setFormat(\"glueparquet\")\ns3output.writeFrame(DyF)",
			"metadata": {
				"editable": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "# Reading data from S3",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "dir_data_customers = r's3://app-planejamento-estrategico/data/tutorial_johnny/customers/customers.csv'",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 11,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Define the schema",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "custom_schema_customers = StructType([\n    StructField(\"customerid\", IntegerType(), True),\n    StructField(\"firstname\", StringType(), True),\n    StructField(\"lastname\", StringType(), True),\n    StructField(\"fullname\", StringType(), True)\n])",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 12,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Reading the file",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "def describe_dataframe(data, n_rows=5):\n    \n    # VALIDATE IF DATA IS PYSPARK DATAFRAME\n    if isinstance(data, pyspark.sql.DataFrame):\n        \n        print(\"PYSPARK DATAFRAME\")\n        print(\"-\"*50)\n        \n        # SHOW DATA\n        print(\"SHOWING DATA\")\n        data.show(n_rows)\n        \n        # COUNT ROWS\n        print(\"COUNTING ROWS\")\n        print(data.count())\n        \n        # LIST COLUMNS\n        print(\"LISTING COLUMNS\")\n        print(data.columns)\n        \n        # DESCRIBING DATA\n        print(\"DESCRIBING DATA\")\n        data.describe().show()\n        \n    elif isinstance(data, pd.DataFrame):\n        \n        print(\"PANDAS DATAFRAME\")\n        print(\"-\"*50)\n        \n        # SHOW DATA\n        print(\"SHOWING DATA\")\n        print(data.head(n_rows))\n        \n        # COUNT ROWS\n        print(\"COUNTING ROWS\")\n        print(len(data))\n        \n        # LIST COLUMNS\n        print(\"LISTING COLUMNS\")\n        print(data.columns)\n        \n        # DESCRIBING DATA\n        print(\"DESCRIBING DATA\")\n        print(data.describe())\n        \n    else:\n        print(\"INVALID DATA TYPE\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 13,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## SPARK",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "df_customers = spark.read.csv(dir_data_customers, \n                              header=True, \n                              schema=custom_schema_customers)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 14,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "describe_dataframe(data=df_customers)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 15,
			"outputs": [
				{
					"name": "stdout",
					"text": "PYSPARK DATAFRAME\n--------------------------------------------------\nSHOWING DATA\n+----------+---------+-----------+----------------+\n|customerid|firstname|   lastname|        fullname|\n+----------+---------+-----------+----------------+\n|       295|      Kim|Abercrombie| Kim Abercrombie|\n|       297| Humberto|    Acevedo|Humberto Acevedo|\n|       291|  Gustavo|     Achong|  Gustavo Achong|\n|       299|    Pilar|   Ackerman|  Pilar Ackerman|\n|       305|    Carla|      Adams|     Carla Adams|\n+----------+---------+-----------+----------------+\nonly showing top 5 rows\n\nCOUNTING ROWS\n634\nLISTING COLUMNS\n['customerid', 'firstname', 'lastname', 'fullname']\nDESCRIBING DATA\n+-------+------------------+---------+-----------+------------+\n|summary|        customerid|firstname|   lastname|    fullname|\n+-------+------------------+---------+-----------+------------+\n|  count|               634|      634|        634|         634|\n|   mean|1039.7917981072555|     null|       null|        null|\n| stddev|473.70467352658903|     null|       null|        null|\n|    min|               291|       A.|Abercrombie| A. Leonetti|\n|    max|              1993|   Yvonne|   Vicknair|Yvonne McKay|\n+-------+------------------+---------+-----------+------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## PANDAS",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "column_names_customers = [field.name for field in custom_schema_customers.fields]",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 16,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df_pandas_customers = pd.read_csv(dir_data_customers, \n                                  header='infer', \n                                  names=column_names_customers)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 17,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "describe_dataframe(data=df_pandas_customers)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 18,
			"outputs": [
				{
					"name": "stdout",
					"text": "PANDAS DATAFRAME\n--------------------------------------------------\nSHOWING DATA\n   customerid  firstname     lastname          fullname\n0         293  Catherine         Abel    Catherine Abel\n1         295        Kim  Abercrombie   Kim Abercrombie\n2         297   Humberto      Acevedo  Humberto Acevedo\n3         291    Gustavo       Achong    Gustavo Achong\n4         299      Pilar     Ackerman    Pilar Ackerman\nCOUNTING ROWS\n635\nLISTING COLUMNS\nIndex(['customerid', 'firstname', 'lastname', 'fullname'], dtype='object')\nDESCRIBING DATA\n        customerid\ncount   635.000000\nmean   1038.615748\nstd     474.257783\nmin     291.000000\n25%     654.000000\n50%     993.000000\n75%    1340.000000\nmax    1993.000000\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Spark - Operations with columns",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "## DEFINING NAME OF COLUMNS USING A LIST",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "df_customers.toDF(*['customerID', 'firstname', 'lastname', 'fullname']).show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 19,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+----------+-----------+--------------------+\n|customerID| firstname|   lastname|            fullname|\n+----------+----------+-----------+--------------------+\n|       295|       Kim|Abercrombie|     Kim Abercrombie|\n|       297|  Humberto|    Acevedo|    Humberto Acevedo|\n|       291|   Gustavo|     Achong|      Gustavo Achong|\n|       299|     Pilar|   Ackerman|      Pilar Ackerman|\n|       305|     Carla|      Adams|         Carla Adams|\n|       301|   Frances|      Adams|       Frances Adams|\n|       307|       Jay|      Adams|           Jay Adams|\n|       309|    Ronald|      Adina|        Ronald Adina|\n|       311|    Samuel|   Agcaoili|     Samuel Agcaoili|\n|       313|     James|    Aguilar|       James Aguilar|\n|       315|    Robert|   Ahlering|     Robert Ahlering|\n|       319|       Kim|      Akers|           Kim Akers|\n|       441|   Stanley|       Alan|        Stanley Alan|\n|       323|       Amy|    Alberts|         Amy Alberts|\n|       325|      Anna|   Albright|       Anna Albright|\n|       327|    Milton|     Albury|       Milton Albury|\n|       329|      Paul|     Alcorn|         Paul Alcorn|\n|       331|   Gregory|   Alderson|    Gregory Alderson|\n|       333|J. Phillip|  Alexander|J. Phillip Alexander|\n|      1149|      Mary|  Alexander|      Mary Alexander|\n+----------+----------+-----------+--------------------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Selecting columns",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "list_columns_to_select = ['customerid', 'firstname', 'lastname']",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 20,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "def filter_columns(data, list_columns):\n    \n    # GET INTERSECTION COLUMNS\n    list_columns_select = [column for column in list_columns_to_select if column in df_customers.columns]\n    \n    if isinstance(data, pyspark.sql.DataFrame):\n        print(\"PYSPARK - FILTERING COLUMNS - {}\".format(list_columns_select))\n        return data.select(*list_columns_select)\n    \n    elif isinstance(data, pd.DataFrame):\n        print(\"PANDAS - FILTERING COLUMNS - {}\".format(list_columns_select))\n        return data[list_columns_select]\n    \n    else:\n        print(\"INVALID DATA TYPE\")\n        return data",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 21,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df_customers_selected = filter_columns(data=df_customers, list_columns=list_columns_to_select)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 22,
			"outputs": [
				{
					"name": "stdout",
					"text": "PYSPARK - FILTERING COLUMNS - ['customerid', 'firstname', 'lastname']\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df_customers_selected.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 23,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+----------+-----------+\n|customerid| firstname|   lastname|\n+----------+----------+-----------+\n|       295|       Kim|Abercrombie|\n|       297|  Humberto|    Acevedo|\n|       291|   Gustavo|     Achong|\n|       299|     Pilar|   Ackerman|\n|       305|     Carla|      Adams|\n|       301|   Frances|      Adams|\n|       307|       Jay|      Adams|\n|       309|    Ronald|      Adina|\n|       311|    Samuel|   Agcaoili|\n|       313|     James|    Aguilar|\n|       315|    Robert|   Ahlering|\n|       319|       Kim|      Akers|\n|       441|   Stanley|       Alan|\n|       323|       Amy|    Alberts|\n|       325|      Anna|   Albright|\n|       327|    Milton|     Albury|\n|       329|      Paul|     Alcorn|\n|       331|   Gregory|   Alderson|\n|       333|J. Phillip|  Alexander|\n|      1149|      Mary|  Alexander|\n+----------+----------+-----------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Current date",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.functions import current_date, date_format",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 24,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "current_date()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 25,
			"outputs": [
				{
					"name": "stdout",
					"text": "Column<'current_date()'>\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Get the current date using the current_date() function\ncurrent_date_df = spark.range(1).select(current_date().alias(\"current_date\"))",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 26,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Show the result\ncurrent_date_df.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 27,
			"outputs": [
				{
					"name": "stdout",
					"text": "+------------+\n|current_date|\n+------------+\n|  2023-08-11|\n+------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Convert the date to string format \"year_month_day\"\ndate_string_df = current_date_df.select(date_format(\"current_date\", \"yyyy_MM_dd\").alias(\"formatted_date\"))",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 28,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Show the result\ndate_string_df.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 29,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------------+\n|formatted_date|\n+--------------+\n|    2023_08_11|\n+--------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Converting types",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "def convert_pyspark_to_pandas(data):\n    \n    if isinstance(data, pyspark.sql.DataFrame):\n        df_converted_pandas = data.toPandas()\n        return df_converted_pandas\n    else:\n        print(\"INVALID DATA TYPE\")\n        return data",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 52,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "from awsglue.dynamicframe import DynamicFrame\n\ndef convert_pyspark_to_dynamicframe(data):\n    \n    if isinstance(data, pyspark.sql.DataFrame):\n        df_converted_dynamic_frame = DynamicFrame.fromDF(data, \n                                                         glueContext, \n                                                         \"pyspark_to_dynamicframe\")\n        return df_converted_dynamic_frame\n    else:\n        print(\"INVALID DATA TYPE\")\n        return data\n    \ndef convert_dynamicframe_to_pyspark(data):\n    \n    if isinstance(data, DynamicFrame):\n        df_converted_pyspark = data.toDF()\n        return df_converted_pyspark\n    else:\n        print(\"INVALID DATA TYPE\")\n        return data",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 54,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "raw",
			"source": "Pyspark to Pyspark: convert_pyspark_to_pandas\nPyspark to DynamicFrame: convert_pyspark_to_dynamicframe\nDynamicFrame to Pyspark: convert_dynamicframe_to_pyspark",
			"metadata": {}
		},
		{
			"cell_type": "markdown",
			"source": "## Pyspark to Pandas",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "df_customers_converted_pandas = convert_pyspark_to_pandas(data=df_customers)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 43,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "type(df_customers_converted_pandas)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 44,
			"outputs": [
				{
					"name": "stdout",
					"text": "<class 'pandas.core.frame.DataFrame'>\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Pyspark to DynamicFrame",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "df_customers_converted_dynamic_frame = convert_pyspark_to_dynamicframe(data=df_customers)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 46,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "type(df_customers_converted_dynamic_frame)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 47,
			"outputs": [
				{
					"name": "stdout",
					"text": "<class 'awsglue.dynamicframe.DynamicFrame'>\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## DynamicFrame to Pyspark",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "df_customers_converted_pyspark = convert_dynamicframe_to_pyspark(df_customers_converted_dynamic_frame)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 56,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "type(df_customers_converted_pyspark)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 57,
			"outputs": [
				{
					"name": "stdout",
					"text": "<class 'pyspark.sql.dataframe.DataFrame'>\n",
					"output_type": "stream"
				}
			]
		}
	]
}