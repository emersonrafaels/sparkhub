{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "#### Optional: Run this cell to see available notebook commands (\"magics\").\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%help",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 0.38.1 \n",
					"output_type": "stream"
				},
				{
					"output_type": "display_data",
					"data": {
						"text/markdown": "\n# Available Magic Commands\n\n## Sessions Magic\n\n----\n    %help                             Return a list of descriptions and input types for all magic commands. \n    %profile            String        Specify a profile in your aws configuration to use as the credentials provider.\n    %region             String        Specify the AWS region in which to initialize a session. \n                                      Default from ~/.aws/config on Linux or macOS, \n                                      or C:\\Users\\ USERNAME \\.aws\\config\" on Windows.\n    %idle_timeout       Int           The number of minutes of inactivity after which a session will timeout. \n                                      Default: 2880 minutes (48 hours).\n    %session_id_prefix  String        Define a String that will precede all session IDs in the format \n                                      [session_id_prefix]-[session_id]. If a session ID is not provided,\n                                      a random UUID will be generated.\n    %status                           Returns the status of the current Glue session including its duration, \n                                      configuration and executing user / role.\n    %session_id                       Returns the session ID for the running session. \n    %list_sessions                    Lists all currently running sessions by ID.\n    %stop_session                     Stops the current session.\n    %glue_version       String        The version of Glue to be used by this session. \n                                      Currently, the only valid options are 2.0, 3.0 and 4.0. \n                                      Default: 2.0.\n----\n\n## Selecting Job Types\n\n----\n    %streaming          String        Sets the session type to Glue Streaming.\n    %etl                String        Sets the session type to Glue ETL.\n    %glue_ray           String        Sets the session type to Glue Ray.\n----\n\n## Glue Config Magic \n*(common across all job types)*\n\n----\n\n    %%configure         Dictionary    A json-formatted dictionary consisting of all configuration parameters for \n                                      a session. Each parameter can be specified here or through individual magics.\n    %iam_role           String        Specify an IAM role ARN to execute your session with.\n                                      Default from ~/.aws/config on Linux or macOS, \n                                      or C:\\Users\\%USERNAME%\\.aws\\config` on Windows.\n    %number_of_workers  int           The number of workers of a defined worker_type that are allocated \n                                      when a session runs.\n                                      Default: 5.\n    %additional_python_modules  List  Comma separated list of additional Python modules to include in your cluster \n                                      (can be from Pypi or S3).\n    %%tags        Dictionary          Specify a json-formatted dictionary consisting of tags to use in the session.\n----\n\n                                      \n## Magic for Spark Jobs (ETL & Streaming)\n\n----\n    %worker_type        String        Set the type of instances the session will use as workers. \n                                      ETL and Streaming support G.1X, G.2X, G.4X and G.8X. \n                                      Default: G.1X.\n    %connections        List          Specify a comma separated list of connections to use in the session.\n    %extra_py_files     List          Comma separated list of additional Python files From S3.\n    %extra_jars         List          Comma separated list of additional Jars to include in the cluster.\n    %spark_conf         String        Specify custom spark configurations for your session. \n                                      E.g. %spark_conf spark.serializer=org.apache.spark.serializer.KryoSerializer\n----\n                                      \n## Magic for Ray Job\n\n----\n    %min_workers        Int           The minimum number of workers that are allocated to a Ray job. \n                                      Default: 1.\n    %object_memory_head Int           The percentage of free memory on the instance head node after a warm start. \n                                      Minimum: 0. Maximum: 100.\n    %object_memory_worker Int         The percentage of free memory on the instance worker nodes after a warm start. \n                                      Minimum: 0. Maximum: 100.\n----\n\n## Action Magic\n\n----\n\n    %%sql               String        Run SQL code. All lines after the initial %%sql magic will be passed\n                                      as part of the SQL code.  \n----\n\n"
					},
					"metadata": {}
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "####  Run this cell to set up and start your interactive session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 2880\n%glue_version 3.0\n%worker_type G.1X\n%number_of_workers 5\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 4e3d3415-d618-4f4d-a5d1-2d8f9a384a17.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Current idle_timeout is 2880 minutes.\nidle_timeout has been set to 2880 minutes.\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 4e3d3415-d618-4f4d-a5d1-2d8f9a384a17.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Setting Glue version to: 3.0\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 4e3d3415-d618-4f4d-a5d1-2d8f9a384a17.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Previous worker type: G.1X\nSetting new worker type to: G.1X\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 4e3d3415-d618-4f4d-a5d1-2d8f9a384a17.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Previous number of workers: 5\nSetting new number of workers to: 5\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "import pyspark\nimport pandas as pd\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 19,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Define the context",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "sc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "raw",
			"source": "#### Example: Create a DynamicFrame from a table in the AWS Glue Data Catalog and display its schema\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "raw",
			"source": "dyf = glueContext.create_dynamic_frame.from_catalog(database='database_name', table_name='table_name')\ndyf.printSchema()",
			"metadata": {
				"editable": true
			}
		},
		{
			"cell_type": "raw",
			"source": "#### Example: Convert the DynamicFrame to a Spark DataFrame and display a sample of the data\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "raw",
			"source": "df = dyf.toDF()\ndf.show()",
			"metadata": {
				"editable": true
			}
		},
		{
			"cell_type": "raw",
			"source": "#### Example: Write the data in the DynamicFrame to a location in Amazon S3 and a table for it in the AWS Glue Data Catalog\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "raw",
			"source": "s3output = glueContext.getSink(\n  path=\"s3://bucket_name/folder_name\",\n  connection_type=\"s3\",\n  updateBehavior=\"UPDATE_IN_DATABASE\",\n  partitionKeys=[],\n  compression=\"snappy\",\n  enableUpdateCatalog=True,\n  transformation_ctx=\"s3output\",\n)\ns3output.setCatalogInfo(\n  catalogDatabase=\"demo\", catalogTableName=\"populations\"\n)\ns3output.setFormat(\"glueparquet\")\ns3output.writeFrame(DyF)",
			"metadata": {
				"editable": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "# Reading data from S3",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "dir_data_customers = r's3://app-planejamento-estrategico/data/tutorial_johnny/customers/customers.csv'",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 42,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Define the schema",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "custom_schema_customers = StructType([\n    StructField(\"customerid\", IntegerType(), True),\n    StructField(\"firstname\", StringType(), True),\n    StructField(\"lastname\", StringType(), True),\n    StructField(\"fullname\", StringType(), True)\n])",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 43,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Reading the file",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "def describe_dataframe(data, n_rows=5):\n    \n    # VALIDATE IF DATA IS PYSPARK DATAFRAME\n    if isinstance(data, pyspark.sql.DataFrame):\n        \n        print(\"PYSPARK DATAFRAME\")\n        print(\"-\"*50)\n        \n        # SHOW DATA\n        print(\"SHOWING DATA\")\n        data.show(n_rows)\n        \n        # COUNT ROWS\n        print(\"COUNTING ROWS\")\n        print(data.count())\n        \n        # LIST COLUMNS\n        print(\"LISTING COLUMNS\")\n        print(data.columns)\n        \n        # DESCRIBING DATA\n        print(\"DESCRIBING DATA\")\n        data.describe().show()\n        \n    elif isinstance(data, pd.DataFrame):\n        \n        print(\"PANDAS DATAFRAME\")\n        print(\"-\"*50)\n        \n        # SHOW DATA\n        print(\"SHOWING DATA\")\n        print(data.head(n_rows))\n        \n        # COUNT ROWS\n        print(\"COUNTING ROWS\")\n        print(len(data))\n        \n        # LIST COLUMNS\n        print(\"LISTING COLUMNS\")\n        print(data.columns)\n        \n        # DESCRIBING DATA\n        print(\"DESCRIBING DATA\")\n        print(data.describe())",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 59,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## SPARK",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "df_customers = spark.read.csv(dir_data_customers, \n                              header=True, \n                              schema=custom_schema_customers)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 45,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "describe_dataframe(data=df_customers)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 57,
			"outputs": [
				{
					"name": "stdout",
					"text": "PYSPARK DATAFRAME\n--------------------------------------------------\nSHOWING DATA\n+----------+---------+-----------+----------------+\n|customerid|firstname|   lastname|        fullname|\n+----------+---------+-----------+----------------+\n|       295|      Kim|Abercrombie| Kim Abercrombie|\n|       297| Humberto|    Acevedo|Humberto Acevedo|\n|       291|  Gustavo|     Achong|  Gustavo Achong|\n|       299|    Pilar|   Ackerman|  Pilar Ackerman|\n|       305|    Carla|      Adams|     Carla Adams|\n+----------+---------+-----------+----------------+\nonly showing top 5 rows\n\nCOUNTING ROWS\n634\nLISTING COLUMNS\n['customerid', 'firstname', 'lastname', 'fullname']\nDESCRIBING DATA\n+-------+------------------+---------+-----------+------------+\n|summary|        customerid|firstname|   lastname|    fullname|\n+-------+------------------+---------+-----------+------------+\n|  count|               634|      634|        634|         634|\n|   mean|1039.7917981072555|     null|       null|        null|\n| stddev|473.70467352658903|     null|       null|        null|\n|    min|               291|       A.|Abercrombie| A. Leonetti|\n|    max|              1993|   Yvonne|   Vicknair|Yvonne McKay|\n+-------+------------------+---------+-----------+------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## PANDAS",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "column_names_customers = [field.name for field in custom_schema_customers.fields]",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 63,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df_pandas_customers = pd.read_csv(dir_data_customers, \n                                  header='infer', \n                                  names=column_names_customers)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 64,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "describe_dataframe(data=df_pandas_customers)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 65,
			"outputs": [
				{
					"name": "stdout",
					"text": "PANDAS DATAFRAME\n--------------------------------------------------\nSHOWING DATA\n   customerid  firstname     lastname          fullname\n0         293  Catherine         Abel    Catherine Abel\n1         295        Kim  Abercrombie   Kim Abercrombie\n2         297   Humberto      Acevedo  Humberto Acevedo\n3         291    Gustavo       Achong    Gustavo Achong\n4         299      Pilar     Ackerman    Pilar Ackerman\nCOUNTING ROWS\n635\nLISTING COLUMNS\nIndex(['customerid', 'firstname', 'lastname', 'fullname'], dtype='object')\nDESCRIBING DATA\n        customerid\ncount   635.000000\nmean   1038.615748\nstd     474.257783\nmin     291.000000\n25%     654.000000\n50%     993.000000\n75%    1340.000000\nmax    1993.000000\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}